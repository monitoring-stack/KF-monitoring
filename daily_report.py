import os
import json
import base64
import urllib.request
import urllib.error
from html import escape
from datetime import datetime

import feedparser
from bs4 import BeautifulSoup

from reportlab.lib.pagesizes import A4
from reportlab.platypus import (
Â Â Â Â SimpleDocTemplate,
Â Â Â Â Paragraph,
Â Â Â Â Spacer,
)
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import mm

from helpers import date_de, classify

# ================== KONFIGURACE ==================

TIMEZONE = os.getenv("TIMEZONE", "Europe/Berlin")

# Resend
RESEND_API_KEY = os.getenv("RESEND_API_KEY")
EMAIL_FROM = os.getenv("EMAIL_FROM")Â Â Â Â Â Â Â Â Â Â # napÅ™. "Kaufland Monitoring <kaufland.monitoring@gmail.com>"
EMAIL_TO = os.getenv("EMAIL_TO")Â Â Â Â Â Â Â Â Â Â Â Â Â Â # hlavnÃ­ pÅ™Ã­jemce (Stefan)
CC = os.getenv("CC")
BCC = os.getenv("BCC")

MAX_TOP = int(os.getenv("MAX_TOP", "10"))

# volitelnÃ½ vstup pro Medium variantu Google Reviews:
# REVIEWS_JSON = JSON pole objektÅ¯:
# [{ "region": "...", "store": "...", "avg": 4.2, "delta": -0.1, "count_24h": 12, "flag": "negativer Trend" }, ...]
REVIEWS_JSON = os.getenv("REVIEWS_JSON", "[]")

FEEDS = [
Â Â Â Â "https://news.google.com/rss/search?q=Kaufland+Deutschland&hl=de&gl=DE&ceid=DE:de",
Â Â Â Â "https://news.google.com/rss/search?q=Kaufland+Filiale&hl=de&gl=DE&ceid=DE:de",
Â Â Â Â "https://news.google.com/rss/search?q=Kaufland+Skandal+OR+R%C3%BCckruf+OR+Boykott&hl=de&gl=DE&ceid=DE:de",
]

# ================== NEWS FETCH ==================


def fetch_news():
Â Â Â Â """NaÄte ÄlÃ¡nky z Google News, vyÄistÃ­ summary a seÅ™adÃ­ podle score (desc)."""
Â Â Â Â seen = set()
Â Â Â Â items = []

Â Â Â Â for url in FEEDS:
Â Â Â Â Â Â Â Â d = feedparser.parse(url)
Â Â Â Â Â Â Â Â for e in d.entries:
Â Â Â Â Â Â Â Â Â Â Â Â link = e.link
Â Â Â Â Â Â Â Â Â Â Â Â if link in seen:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â continue
Â Â Â Â Â Â Â Â Â Â Â Â seen.add(link)

Â Â Â Â Â Â Â Â Â Â Â Â title = e.title or ""
Â Â Â Â Â Â Â Â Â Â Â Â desc = BeautifulSoup(getattr(e, "summary", "") or "", "html.parser").get_text()
Â Â Â Â Â Â Â Â Â Â Â Â host, typ, score = classify(link, title)

Â Â Â Â Â Â Â Â Â Â Â Â # filtrujeme na Kaufland
Â Â Â Â Â Â Â Â Â Â Â Â text = f"{title} {desc}".lower()
Â Â Â Â Â Â Â Â Â Â Â Â if "kaufland" not in text:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â continue

Â Â Â Â Â Â Â Â Â Â Â Â items.append(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "title": title.strip(),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "url": link,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "summary": desc.strip(),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "source": host,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "type": typ,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "score": score,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "why": "relevant" if score >= 4 else "beobachten",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â Â Â Â Â )

Â Â Â Â # seÅ™adit podle score (nejdÅ™Ã­v nejdÅ¯leÅ¾itÄ›jÅ¡Ã­)
Â Â Â Â items.sort(key=lambda x: x["score"], reverse=True)
Â Â Â Â return items


# ================== KLASIFIKACE DE vs INTERNATIONAL ==================

DE_KEYWORDS = [
Â Â Â Â "deutschland",
Â Â Â Â "bundesweit",
Â Â Â Â "berlin",
Â Â Â Â "hamburg",
Â Â Â Â "mÃ¼nchen",
Â Â Â Â "kÃ¶ln",
Â Â Â Â "frankfurt",
Â Â Â Â "stuttgart",
Â Â Â Â "leipzig",
Â Â Â Â "nÃ¼rnberg",
Â Â Â Â "kaufland deutschland",
]


def is_de_article(item):
Â Â Â Â """HrubÃ¡ heuristika, zda jde o DE ÄlÃ¡nek."""
Â Â Â Â host = item.get("source", "").lower()
Â Â Â Â title = item.get("title", "").lower()
Â Â Â Â summary = item.get("summary", "").lower()

Â Â Â Â if host.endswith(".de"):
Â Â Â Â Â Â Â Â return True

Â Â Â Â text = f"{title} {summary}"
Â Â Â Â for kw in DE_KEYWORDS:
Â Â Â Â Â Â Â Â if kw in text:
Â Â Â Â Â Â Â Â Â Â Â Â return True

Â Â Â Â return False


def split_for_email(items_sorted, max_top):
Â Â Â Â """
Â Â Â Â Z items seÅ™azenÃ½ch podle score udÄ›lÃ¡ tÅ™i seznamy bez duplicit URL:
Â Â Â Â - top_de: max_top nejlepÅ¡Ã­ch nÄ›meckÃ½ch ÄlÃ¡nkÅ¯
Â Â Â Â - other_de: ostatnÃ­ DE ÄlÃ¡nky
Â Â Â Â - intl: international ÄlÃ¡nky
Â Â Â Â """
Â Â Â Â top_de = []
Â Â Â Â other_de = []
Â Â Â Â intl = []
Â Â Â Â seen_urls = set()

Â Â Â Â for it in items_sorted:
Â Â Â Â Â Â Â Â url = it["url"]
Â Â Â Â Â Â Â Â if url in seen_urls:
Â Â Â Â Â Â Â Â Â Â Â Â continue
Â Â Â Â Â Â Â Â seen_urls.add(url)

Â Â Â Â Â Â Â Â if is_de_article(it):
Â Â Â Â Â Â Â Â Â Â Â Â if len(top_de) < max_top:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â top_de.append(it)
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â other_de.append(it)
Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â intl.append(it)

Â Â Â Â return top_de, other_de, intl


# ================== GOOGLE REVIEWS (MIN + MEDIUM) ==================


def get_google_reviews_data():
Â Â Â Â """
Â Â Â Â MIN varianta:
Â Â Â Â Â Â - pokud REVIEWS_JSON nenÃ­ vyplnÄ›nÃ© â†’ vrÃ¡tÃ­ prÃ¡zdnÃ½ list â†’ zobrazÃ­ se vysvÄ›tlujÃ­cÃ­ Å™Ã¡dek.

Â Â Â Â MEDIUM varianta:
Â Â Â Â Â Â - pokud REVIEWS_JSON obsahuje JSON seznam objektÅ¯:
Â Â Â Â Â Â Â Â {region, store, avg, delta, count_24h, flag}
Â Â Â Â Â Â Â Â â†’ seÅ™adÃ­ podle 'priority' a vrÃ¡tÃ­ TOP 5.
Â Â Â Â """
Â Â Â Â try:
Â Â Â Â Â Â Â Â raw = REVIEWS_JSON.strip()
Â Â Â Â Â Â Â Â if not raw or raw == "[]":
Â Â Â Â Â Â Â Â Â Â Â Â return []
Â Â Â Â Â Â Â Â data = json.loads(raw)
Â Â Â Â Â Â Â Â if not isinstance(data, list):
Â Â Â Â Â Â Â Â Â Â Â Â return []
Â Â Â Â except Exception:
Â Â Â Â Â Â Â Â return []

Â Â Â Â def prio(r):
Â Â Â Â Â Â Â Â delta = abs(r.get("delta") or 0.0)
Â Â Â Â Â Â Â Â count = r.get("count_24h") or 0
Â Â Â Â Â Â Â Â # jednoduchÃ© skÃ³re: velkÃ¡ zmÄ›na ratingu + hodnÄ› novÃ½ch recenzÃ­
Â Â Â Â Â Â Â Â return delta * 10 + count

Â Â Â Â data_sorted = sorted(data, key=prio, reverse=True)
Â Â Â Â return data_sorted[:5]


# ================== PDF â€“ MAGAZINE LAYOUT ==================


def shorten_url(url: str, max_len: int = 50) -> str:
Â Â Â Â """ZkrÃ¡cenÃ¡ URL pro zobrazenÃ­ v PDF."""
Â Â Â Â if not url:
Â Â Â Â Â Â Â Â return ""
Â Â Â Â # odÅ™Ã­zneme protokol
Â Â Â Â u = url.replace("https://", "").replace("http://", "")
Â Â Â Â if len(u) <= max_len:
Â Â Â Â Â Â Â Â return u
Â Â Â Â return u[: max_len - 1] + "â€¦"


def build_pdf_magazine(filename, top_de, other_de, intl):
Â Â Â Â """
Â Â Â Â VytvoÅ™Ã­ â€magazine styleâ€œ PDF:
Â Â Â Â Â Â - Å¾Ã¡dnÃ¡ velkÃ¡ tabulka
Â Â Â Â Â Â - sekce:
Â Â Â Â Â Â Â Â * Top Schlagzeilen (DE)
Â Â Â Â Â Â Â Â * Virale ErwÃ¤hnungen (DE)
Â Â Â Â Â Â Â Â * International â€“ Virale ErwÃ¤hnungen
Â Â Â Â Â Â - kaÅ¾dÃ½ ÄlÃ¡nek jako blok: #, titulek, meta, summary, link
Â Â Â Â """
Â Â Â Â styles = getSampleStyleSheet()

Â Â Â Â title_style = styles["Title"]
Â Â Â Â h2 = styles["Heading2"]
Â Â Â Â meta_style = ParagraphStyle(
Â Â Â Â Â Â Â Â "Meta",
Â Â Â Â Â Â Â Â parent=styles["Normal"],
Â Â Â Â Â Â Â Â fontSize = 9,
Â Â Â Â Â Â Â Â textColor = "grey",
Â Â Â Â )
Â Â Â Â body_style = ParagraphStyle(
Â Â Â Â Â Â Â Â "Body",
Â Â Â Â Â Â Â Â parent=styles["Normal"],
Â Â Â Â Â Â Â Â fontSize = 10,
Â Â Â Â Â Â Â Â leading = 13,
Â Â Â Â )
Â Â Â Â link_style = ParagraphStyle(
Â Â Â Â Â Â Â Â "Link",
Â Â Â Â Â Â Â Â parent=styles["Normal"],
Â Â Â Â Â Â Â Â fontSize = 8.5,
Â Â Â Â Â Â Â Â textColor = "blue",
Â Â Â Â )

Â Â Â Â doc = SimpleDocTemplate(
Â Â Â Â Â Â Â Â filename,
Â Â Â Â Â Â Â Â pagesize=A4,
Â Â Â Â Â Â Â Â leftMargin=18 * mm,
Â Â Â Â Â Â Â Â rightMargin=18 * mm,
Â Â Â Â Â Â Â Â topMargin=16 * mm,
Â Â Â Â Â Â Â Â bottomMargin=16 * mm,
Â Â Â Â )

Â Â Â Â story = []

Â Â Â Â # Titulek + datum
Â Â Â Â story.append(Paragraph("Kaufland Media & Review Briefing â€“ Deutschland", title_style))
Â Â Â Â story.append(Spacer(1, 6))
Â Â Â Â story.append(Paragraph(date_de(TIMEZONE), meta_style))
Â Â Â Â story.append(Spacer(1, 14))

Â Â Â Â # --- Top Schlagzeilen (DE) ---
Â Â Â Â if top_de:
Â Â Â Â Â Â Â Â story.append(Paragraph("Top Schlagzeilen (DE)", h2))
Â Â Â Â Â Â Â Â story.append(
Â Â Â Â Â Â Â Â Â Â Â Â Paragraph(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "Priorisiert nach internem Relevanz-/Risiko-Score (nicht echte Reichweite).",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â meta_style,
Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â story.append(Spacer(1, 8))

Â Â Â Â Â Â Â Â for idx, it in enumerate(top_de, start=1):
Â Â Â Â Â Â Â Â Â Â Â Â story.append(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Paragraph(f"{idx}. {escape(it['title'])}", styles["Heading3"])
Â Â Â Â Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â Â Â Â Â meta_parts = []
Â Â Â Â Â Â Â Â Â Â Â Â if it.get("source"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["source"]))
Â Â Â Â Â Â Â Â Â Â Â Â if it.get("type"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["type"]))
Â Â Â Â Â Â Â Â Â Â Â Â if it.get("why"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append("Grund: " + escape(it["why"]))
Â Â Â Â Â Â Â Â Â Â Â Â if meta_parts:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â story.append(Paragraph(" Â· ".join(meta_parts), meta_style))

Â Â Â Â Â Â Â Â Â Â Â Â if it.get("summary"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â story.append(Paragraph(escape(it["summary"]), body_style))

Â Â Â Â Â Â Â Â Â Â Â Â if it.get("url"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â short = shorten_url(it["url"])
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â story.append(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Paragraph(
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â f"<link href='{it['url']}' color='blue'>{escape(short)}</link>",
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â link_style,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â )
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â Â Â Â Â story.append(Spacer(1, 10))

Â Â Â Â # --- Virale ErwÃ¤hnungen (DE) ---
Â Â Â Â if other_de:
Â Â Â Â Â Â Â Â story.append(Spacer(1, 16))
Â Â Â Â Â Â Â Â story.append(Paragraph("Virale ErwÃ¤hnungen (DE)", h2))
Â Â Â Â Â Â Â Â story.append(Spacer(1, 6))

Â Â Â Â Â Â Â Â for it in other_de:
Â Â Â Â Â Â Â Â Â Â Â Â title = escape(it["title"])
Â Â Â Â Â Â Â Â Â Â Â Â src = escape(it.get("source", ""))
Â Â Â Â Â Â Â Â Â Â Â Â line = f"â€¢ {title} ({src})"
Â Â Â Â Â Â Â Â Â Â Â Â story.append(Paragraph(line, body_style))

Â Â Â Â Â Â Â Â story.append(Spacer(1, 10))

Â Â Â Â # --- International â€“ Virale ErwÃ¤hnungen ---
Â Â Â Â if intl:
Â Â Â Â Â Â Â Â story.append(Spacer(1, 16))
Â Â Â Â Â Â Â Â story.append(Paragraph("International â€“ Virale ErwÃ¤hnungen", h2))
Â Â Â Â Â Â Â Â story.append(Spacer(1, 6))

Â Â Â Â Â Â Â Â for it in intl:
Â Â Â Â Â Â Â Â Â Â Â Â title = escape(it["title"])
Â Â Â Â Â Â Â Â Â Â Â Â src = escape(it.get("source", ""))
Â Â Â Â Â Â Â Â Â Â Â Â line = f"â€¢ {title} ({src})"
Â Â Â Â Â Â Â Â Â Â Â Â story.append(Paragraph(line, body_style))

Â Â Â Â doc.build(story)


# ================== RESEND EMAIL ==================


def send_via_resend(subject, html, pdf_name):
Â Â Â Â if not RESEND_API_KEY:
Â Â Â Â Â Â Â Â raise RuntimeError("RESEND_API_KEY env variable is missing.")
Â Â Â Â if not EMAIL_FROM:
Â Â Â Â Â Â Â Â raise RuntimeError("EMAIL_FROM env variable is missing.")
Â Â Â Â if not EMAIL_TO:
Â Â Â Â Â Â Â Â raise RuntimeError("EMAIL_TO env variable is missing.")

Â Â Â Â with open(pdf_name, "rb") as f:
Â Â Â Â Â Â Â Â pdf_bytes = f.read()
Â Â Â Â pdf_b64 = base64.b64encode(pdf_bytes).decode("ascii")

Â Â Â Â payload = {
Â Â Â Â Â Â Â Â "from": EMAIL_FROM,
Â Â Â Â Â Â Â Â "to": [EMAIL_TO],
Â Â Â Â Â Â Â Â "subject": subject,
Â Â Â Â Â Â Â Â "html": html,
Â Â Â Â Â Â Â Â "attachments": [
Â Â Â Â Â Â Â Â Â Â Â Â {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "filename": os.path.basename(pdf_name),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "content": pdf_b64,
Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â ],
Â Â Â Â }

Â Â Â Â if CC:
Â Â Â Â Â Â Â Â payload["cc"] = [x.strip() for x in CC.split(",") if x.strip()]

Â Â Â Â if BCC:
Â Â Â Â Â Â Â Â payload["bcc"] = [x.strip() for x in BCC.split(",") if x.strip()]

Â Â Â Â data_bytes = json.dumps(payload).encode("utf-8")

Â Â Â Â req = urllib.request.Request(
Â Â Â Â Â Â Â Â "https://api.resend.com/emails",
Â Â Â Â Â Â Â Â data=data_bytes,
Â Â Â Â Â Â Â Â headers={
Â Â Â Â Â Â Â Â Â Â Â Â "Authorization": f"Bearer {RESEND_API_KEY}",
Â Â Â Â Â Â Â Â Â Â Â Â "Content-Type": "application/json",
Â Â Â Â Â Â Â Â },
Â Â Â Â Â Â Â Â method="POST",
Â Â Â Â )

Â Â Â Â try:
Â Â Â Â Â Â Â Â with urllib.request.urlopen(req) as resp:
Â Â Â Â Â Â Â Â Â Â Â Â body = resp.read().decode("utf-8")
Â Â Â Â Â Â Â Â Â Â Â Â print("Resend response:", resp.status, body)
Â Â Â Â except urllib.error.HTTPError as e:
Â Â Â Â Â Â Â Â print("Resend HTTP error:", e.code, e.read().decode("utf-8"))
Â Â Â Â Â Â Â Â raise
Â Â Â Â except urllib.error.URLError as e:
Â Â Â Â Â Â Â Â print("Resend connection error:", e.reason)
Â Â Â Â Â Â Â Â raise


# ================== MAIN ==================


def main():
Â Â Â Â # News
Â Â Â Â items = fetch_news()
Â Â Â Â top_de, other_de, intl = split_for_email(items, MAX_TOP)

Â Â Â Â # Google Reviews data (min/medium)
Â Â Â Â reviews = get_google_reviews_data()

Â Â Â Â # HTML Å¡ablona
Â Â Â Â with open("email_template.html", "r", encoding="utf-8") as f:
Â Â Â Â Â Â Â Â template_str = f.read()

Â Â Â Â # Executive summary (DE)
Â Â Â Â executive_summary_html = """
<p><strong>Insight:</strong> Kuratierte Top-Schlagzeilen (1â€“10) fÃ¼r Deutschland; weitere ErwÃ¤hnungen und internationale Hinweise unten.</p>
<p><strong>Implikation:</strong> Schneller Ãœberblick Ã¼ber Themen, Risiken und regionale Besonderheiten in einem tÃ¤glichen Briefing.</p>
<p><strong>Aktion:</strong> Google Reviews sind im Pilotmodus angebunden; mit REVIEWS_JSON kÃ¶nnen Filialen mit auffÃ¤lligen Trends hervorgehoben werden.</p>
""".strip()

Â Â Â Â # Top Schlagzeilen â€“ HTML
Â Â Â Â top_items_html = []
Â Â Â Â for i, it in enumerate(top_de, start=1):
Â Â Â Â Â Â Â Â meta_parts = []
Â Â Â Â Â Â Â Â if it.get("source"):
Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["source"]))
Â Â Â Â Â Â Â Â if it.get("why"):
Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append("Grund: " + escape(it["why"]))
Â Â Â Â Â Â Â Â meta = " Â· ".join(meta_parts)

Â Â Â Â Â Â Â Â top_items_html.append(
Â Â Â Â Â Â Â Â Â Â Â Â f"""
<li class="item">
Â Â <span class="rank">{i}.</span>
Â Â <span>
Â Â Â Â <a href="{it['url']}">{escape(it['title'])}</a>
Â Â Â Â <div class="meta">{meta}</div>
Â Â </span>
</li>""".strip()
Â Â Â Â Â Â Â Â )

Â Â Â Â # Virale ErwÃ¤hnungen (DE) â€“ HTML
Â Â Â Â other_de_html = []
Â Â Â Â for it in other_de:
Â Â Â Â Â Â Â Â meta_parts = []
Â Â Â Â Â Â Â Â if it.get("source"):
Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["source"]))
Â Â Â Â Â Â Â Â if it.get("type"):
Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["type"]))
Â Â Â Â Â Â Â Â meta = " Â· ".join(meta_parts)

Â Â Â Â Â Â Â Â other_de_html.append(
Â Â Â Â Â Â Â Â Â Â Â Â f"""
<li class="item">
Â Â <span class="rank">â€¢</span>
Â Â <span>
Â Â Â Â <a href="{it['url']}">{escape(it['title'])}</a>
Â Â Â Â <div class="meta">{meta}</div>
Â Â </span>
</li>""".strip()
Â Â Â Â Â Â Â Â )

Â Â Â Â # International â€“ HTML
Â Â Â Â international_items_html = []
Â Â Â Â for it in intl:
Â Â Â Â Â Â Â Â meta_parts = []
Â Â Â Â Â Â Â Â if it.get("source"):
Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["source"]))
Â Â Â Â Â Â Â Â if it.get("type"):
Â Â Â Â Â Â Â Â Â Â Â Â meta_parts.append(escape(it["type"]))
Â Â Â Â Â Â Â Â meta = " Â· ".join(meta_parts)

Â Â Â Â Â Â Â Â international_items_html.append(
Â Â Â Â Â Â Â Â Â Â Â Â f"""
<li class="item">
Â Â <span class="rank">â€¢</span>
Â Â <span>
Â Â Â Â <a href="{it['url']}">{escape(it['title'])}</a>
Â Â Â Â <div class="meta">{meta}</div>
Â Â </span>
</li>""".strip()
Â Â Â Â Â Â Â Â )

Â Â Â Â if not international_items_html:
Â Â Â Â Â Â Â Â international_items_html.append(
Â Â Â Â Â Â Â Â Â Â Â Â "<li class='item'><span>Heute keine relevanten internationalen ErwÃ¤hnungen.</span></li>"
Â Â Â Â Â Â Â Â )

Â Â Â Â # Google Reviews â€“ tabulka HTML
Â Â Â Â review_rows = []
Â Â Â Â for r in reviews:
Â Â Â Â Â Â Â Â delta = r.get("delta")
Â Â Â Â Â Â Â Â delta_class = "pos" if (delta is not None and delta >= 0) else "neg"
Â Â Â Â Â Â Â Â review_rows.append(
Â Â Â Â Â Â Â Â Â Â Â Â f"""
<tr>
Â Â <td>{escape(r.get('region','â€“'))} â€“ {escape(r.get('store','â€“'))}</td>
Â Â <td>{r.get('avg','â€“')}</td>
Â Â <td class="{delta_class}">{delta if delta is not None else 'â€“'}</td>
Â Â <td>{r.get('count_24h','â€“')}</td>
Â Â <td>{escape(r.get('flag','â€“'))}</td>
</tr>""".strip()
Â Â Â Â Â Â Â Â )

Â Â Â Â if not review_rows:
Â Â Â Â Â Â Â Â review_rows = [
Â Â Â Â Â Â Â Â Â Â Â Â """<tr><td colspan="5" class="muted">
Noch keine Filial-spezifischen Daten hinterlegt (Pilotmodus). 
Ãœber REVIEWS_JSON kÃ¶nnen Filialen mit vielen neuen oder auffÃ¤lligen Bewertungen eingebunden werden.
</td></tr>"""
Â Â Â Â Â Â Â Â ]

Â Â Â Â reviews_note = "Î” = VerÃ¤nderung der Ã˜-Bewertung in den letzten 24 Stunden (sofern Daten vorliegen)."

Â Â Â Â # Urgent / Rumors â€“ zatÃ­m prÃ¡zdnÃ©
Â Â Â Â urgent_block_html = ""
Â Â Â Â rumors_block_html = ""

Â Â Â Â # Replace do Å¡ablony
Â Â Â Â html = template_str
Â Â Â Â replacements = {
Â Â Â Â Â Â Â Â "{date_str}": date_de(TIMEZONE),
Â Â Â Â Â Â Â Â "{tz}": TIMEZONE,
Â Â Â Â Â Â Â Â "{recipient}": EMAIL_TO or "",
Â Â Â Â Â Â Â Â "{executive_summary_html}": executive_summary_html,
Â Â Â Â Â Â Â Â "{top_count}": str(len(top_de)),
Â Â Â Â Â Â Â Â "{top_headlines_html}": "\n".join(top_items_html),
Â Â Â Â Â Â Â Â "{other_de_html}": "\n".join(other_de_html),
Â Â Â Â Â Â Â Â "{reviews_table_rows_html}": "\n".join(review_rows),
Â Â Â Â Â Â Â Â "{reviews_note}": reviews_note,
Â Â Â Â Â Â Â Â "{urgent_block_html}": urgent_block_html,
Â Â Â Â Â Â Â Â "{rumors_block_html}": rumors_block_html,
Â Â Â Â Â Â Â Â "{international_html}": "\n".join(international_items_html),
Â Â Â Â }
Â Â Â Â for key, val in replacements.items():
Â Â Â Â Â Â Â Â html = html.replace(key, val)

Â Â Â Â # PDF + odeslÃ¡nÃ­
Â Â Â Â pdf_name = f"DE_monitoring_privat_{datetime.now().strftime('%Y-%m-%d')}.pdf"
Â Â Â Â build_pdf_magazine(pdf_name, top_de, other_de, intl)

Â Â Â Â subject = f"ğŸ“° Kaufland Media & Review Briefing | {date_de(TIMEZONE)}"
Â Â Â Â send_via_resend(subject, html, pdf_name)


if __name__ == "__main__":
Â Â Â Â main()
